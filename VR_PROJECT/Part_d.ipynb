{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7_hn9uQNaTqu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\roopa\\AppData\\Local\\Temp\\ipykernel_17472\\1125880833.py:167: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Initializing gradient scaler for mixed precision\n",
            "c:\\Users\\roopa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Importing essential PyTorch libraries for deep learning operations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Importing mixed precision training utilities for enhanced performance\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Implementing the DoubleConv block that forms the fundamental building block of U-Net\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"Defining a double convolution block with (Conv -> BatchNorm -> ReLU) repeated twice\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        # Constructing sequential convolution operations\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Processing input through the convolution block\n",
        "        return self.conv(x)\n",
        "\n",
        "# Implementing the complete U-Net architecture\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Constructing the encoder (downsampling path)\n",
        "        self.enc1 = DoubleConv(in_channels, 64)  # First encoding block\n",
        "        self.enc2 = DoubleConv(64, 128)         # Second encoding block\n",
        "        self.enc3 = DoubleConv(128, 256)        # Third encoding block\n",
        "        self.enc4 = DoubleConv(256, 512)        # Fourth encoding block\n",
        "        self.enc5 = DoubleConv(512, 1024)       # Bottleneck layer\n",
        "\n",
        "        # Defining max pooling operation for spatial dimension reduction\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Constructing the decoder (upsampling path) with skip connections\n",
        "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)  # First upsampling\n",
        "        self.dec4 = DoubleConv(1024, 512)       # First decoding block\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)   # Second upsampling\n",
        "        self.dec3 = DoubleConv(512, 256)        # Second decoding block\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)   # Third upsampling\n",
        "        self.dec2 = DoubleConv(256, 128)        # Third decoding block\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)    # Fourth upsampling\n",
        "        self.dec1 = DoubleConv(128, 64)         # Fourth decoding block\n",
        "\n",
        "        # Final convolution layer producing the segmentation output\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implementing the forward pass through the U-Net architecture\n",
        "        \n",
        "        # Processing through the encoder path\n",
        "        x1 = self.enc1(x)          # First encoding block\n",
        "        x2 = self.enc2(self.pool(x1))  # Second encoding block with downsampling\n",
        "        x3 = self.enc3(self.pool(x2))  # Third encoding block with downsampling\n",
        "        x4 = self.enc4(self.pool(x3))  # Fourth encoding block with downsampling\n",
        "        x5 = self.enc5(self.pool(x4))  # Bottleneck layer with maximum downsampling\n",
        "\n",
        "        # Processing through the decoder path with skip connections\n",
        "        d4 = self.up4(x5)           # First upsampling operation\n",
        "        d4 = torch.cat((d4, x4), dim=1)  # Incorporating skip connection\n",
        "        d4 = self.dec4(d4)          # First decoding block\n",
        "\n",
        "        d3 = self.up3(d4)           # Second upsampling operation\n",
        "        d3 = torch.cat((d3, x3), dim=1)  # Incorporating skip connection\n",
        "        d3 = self.dec3(d3)          # Second decoding block\n",
        "\n",
        "        d2 = self.up2(d3)           # Third upsampling operation\n",
        "        d2 = torch.cat((d2, x2), dim=1)  # Incorporating skip connection\n",
        "        d2 = self.dec2(d2)          # Third decoding block\n",
        "\n",
        "        d1 = self.up1(d2)           # Fourth upsampling operation\n",
        "        d1 = torch.cat((d1, x1), dim=1)  # Incorporating skip connection\n",
        "        d1 = self.dec1(d1)          # Fourth decoding block\n",
        "\n",
        "        # Generating final segmentation output\n",
        "        return self.final_conv(d1)\n",
        "\n",
        "# Implementing custom dataset loader for image-mask pairs\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        # Listing all images in the face_crop directory\n",
        "        self.images = os.listdir(os.path.join(root_dir, 'face_crop'))\n",
        "        # Verifying corresponding masks exist for each image\n",
        "        self.images = [img for img in self.images if os.path.exists(os.path.join(root_dir, 'face_crop_segmentation', img))]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returning total number of valid image-mask pairs\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Constructing paths to image and mask files\n",
        "        img_name = os.path.join(self.root_dir, 'face_crop', self.images[idx])\n",
        "        mask_name = os.path.join(self.root_dir, 'face_crop_segmentation', self.images[idx])\n",
        "\n",
        "        # Loading image and mask\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        mask = Image.open(mask_name).convert('L')\n",
        "\n",
        "        # Applying transformations if specified\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Implementing Intersection over Union (IoU) metric calculation\n",
        "def iou_score(preds, targets, threshold=0.5):\n",
        "    # Converting predictions to binary masks using threshold\n",
        "    preds = torch.sigmoid(preds) > threshold\n",
        "    targets = targets > threshold\n",
        "    \n",
        "    # Calculating intersection and union areas\n",
        "    intersection = (preds & targets).float().sum((1, 2, 3))\n",
        "    union = (preds | targets).float().sum((1, 2, 3))\n",
        "    \n",
        "    # Computing IoU while preventing division by zero\n",
        "    iou = (intersection / (union + 1e-6)).mean()\n",
        "    return iou.item()\n",
        "\n",
        "# Defining image preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resizing images to consistent dimensions\n",
        "    transforms.ToTensor(),          # Converting images to tensor format\n",
        "])\n",
        "  \n",
        "# Determining base directory path\n",
        "base_dir = os.getcwd() \n",
        "\n",
        "# Constructing dataset path  \n",
        "dataset_path = os.path.join(base_dir, \"MSFD\", \"1\")  \n",
        "\n",
        "# Loading and splitting dataset  \n",
        "full_dataset = CustomDataset(root_dir=dataset_path, transform=transform)  \n",
        "train_size = int(0.8 * len(full_dataset))  # Allocating 80% for training\n",
        "test_size = len(full_dataset) - train_size  # Allocating 20% for testing\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])  \n",
        "\n",
        "# Creating data loaders for efficient batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Determining computation device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initializing model components\n",
        "model = UNet(in_channels=3, out_channels=1).to(device)  # Instantiating U-Net model\n",
        "criterion = nn.BCEWithLogitsLoss()  # Defining loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)  # Configuring optimizer\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)  # Setting learning rate scheduler\n",
        "scaler = GradScaler()  # Initializing gradient scaler for mixed precision\n",
        "\n",
        "# Implementing training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Setting model to training mode\n",
        "    train_loss = 0\n",
        "    train_iou = 0\n",
        "    \n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Transferring data to computation device\n",
        "        data, targets = data.to(device), targets.float().to(device)\n",
        "        \n",
        "        # Resetting gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Performing forward pass with mixed precision\n",
        "        with autocast():\n",
        "            scores = model(data)  # Computing predictions\n",
        "            loss = criterion(scores, targets)  # Calculating loss\n",
        "\n",
        "        # Performing backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulating training metrics\n",
        "        train_loss += loss.item()\n",
        "        train_iou += iou_score(scores, targets)\n",
        "\n",
        "        # Printing batch progress\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}, IoU: {train_iou / (batch_idx + 1):.4f}\")\n",
        "\n",
        "    # Updating learning rate\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Calculating epoch averages\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_train_iou = train_iou / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Train IoU: {avg_train_iou:.4f}\")\n",
        "\n",
        "# Saving trained model weights\n",
        "base_dir=os.getcwd()\n",
        "model_path = os.path.join(base_dir, \"standard_unet.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Implementing model evaluation function\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()  # Setting model to evaluation mode\n",
        "    test_iou = 0\n",
        "\n",
        "    # Disabling gradient computation for evaluation\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.float().to(device)\n",
        "\n",
        "            scores = model(data)  # Computing predictions\n",
        "            iou = iou_score(scores, targets)  # Calculating IoU\n",
        "            test_iou += iou\n",
        "\n",
        "    # Computing average test IoU\n",
        "    avg_test_iou = test_iou / len(test_loader)\n",
        "    return avg_test_iou\n",
        "\n",
        "# Uncomment to perform evaluation\n",
        "# test_iou = evaluate_model(model, test_loader, device)\n",
        "# print(f\"Test IoU Score: {test_iou:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()  # Setting model to evaluation mode\n",
        "    test_iou = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation needed for evaluation\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.float().to(device)\n",
        "\n",
        "            scores = model(data)  # Forward pass\n",
        "            iou = iou_score(scores, targets)  # Computting the IoU scores\n",
        "            test_iou += iou\n",
        "\n",
        "    avg_test_iou = test_iou / len(test_loader)  # Computing the average IoU scores\n",
        "    return avg_test_iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yHp3p36uiKS9",
        "outputId": "e931678e-5654-4d43-d51a-8ac49974fcc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (enc1): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (enc2): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (enc3): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (enc4): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (enc5): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (up4): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (dec4): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (up3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (dec3): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (up2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (dec2): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (up1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (dec1): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the Pretrained Model\n",
        "model = UNet(in_channels=3, out_channels=1).to(device)\n",
        "model_path=os.path.join(base_dir, \"standard_unet.pth\")\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()  # Setting the model to evaluation mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neK-xbTuigaE",
        "outputId": "0b3601b2-cc4d-47e9-8f61-8830271be873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test IoU Score: 0.9573\n"
          ]
        }
      ],
      "source": [
        "# Printing the final results\n",
        "test_iou = evaluate_model(model, test_loader, device)\n",
        "print(f\"Test IoU Score: {test_iou:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
